{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and converting KiDS KV450 data to Sacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacc\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we download the data from the kids website\n",
    "\n",
    "The old CFHTLenS data sets seem not to be up any more, but hopefully this will stay up longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory already downloaded\n"
     ]
    }
   ],
   "source": [
    "dirname = 'KiDS-450_COSMIC_SHEAR_DATA_RELEASE'\n",
    "filename = dirname + '.tar.gz'\n",
    "url = f'http://kids.strw.leidenuniv.nl/cs2016/{filename}'\n",
    "\n",
    "# Check if already downloaded\n",
    "if os.path.exists(dirname):\n",
    "    print(\"Data directory already downloaded\")\n",
    "else:\n",
    "    print(\"Downloading file\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    # Unpack and remove tgz file\n",
    "    print(\"Unpacking file\")\n",
    "    os.system('tar -xvf ' + filename)\n",
    "    os.system('rm ' + filename)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we make a new empty Sacc and gradually fill it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sacc.Sacc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First with the n(z) data\n",
    "\n",
    "These are fairly straightforward.  We use the extra_columns feature to save the n(z) error.\n",
    "Note that this is the statistical error only, not the systematics.\n",
    "\n",
    "There are 4 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_files = [\n",
    "\"Nz_DIR_z0.1t0.3.asc\",\n",
    "\"Nz_DIR_z0.3t0.5.asc\",\n",
    "\"Nz_DIR_z0.5t0.7.asc\",\n",
    "\"Nz_DIR_z0.7t0.9.asc\",\n",
    "]\n",
    "\n",
    "for i, nz_file in enumerate(nz_files):\n",
    "    # KiDS tomographic bins are numbered from 1\n",
    "    i += 1\n",
    "\n",
    "    # Load n(z)\n",
    "    z, nz, err_z = np.loadtxt(f'KiDS-450_COSMIC_SHEAR_DATA_RELEASE/Nz_DIR/Nz_DIR_Mean/{nz_file}').T\n",
    "    \n",
    "    # Sacc wants bin centers, whereas this file as bin edges, so convert\n",
    "    z += 0.5*(z[1]-z[0])\n",
    "    \n",
    "    S.add_tracer('NZ', f'bin_{i}', z, nz, extra_columns={'err_z': err_z})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the data vector itself\n",
    "\n",
    "The xi_plus and xi_minus data are indicated by a 1 or a 2.\n",
    "\n",
    "Normally we would use astropy to read this, but the header line in the data file has a missing space that messes this up, as the number of columns doesn't match it.\n",
    "\n",
    "So we will parse it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('KiDS-450_COSMIC_SHEAR_DATA_RELEASE/DATA_VECTOR/KiDS-450_xi_pm_tomographic_data_vector.dat')\n",
    "\n",
    "for line in data_file:\n",
    "    line = line.strip()\n",
    "\n",
    "    # skip blank and comment lines\n",
    "    if line.startswith('#') or not line:\n",
    "        continue\n",
    "\n",
    "    # Parse the line\n",
    "    _, theta, xi, type_indicator, i_bin, j_bin = line.split()\n",
    "\n",
    "    theta = float(theta)\n",
    "    xi = float(xi)\n",
    "    \n",
    "    # Use the standard types from sacc to indicate types\n",
    "    if type_indicator == '1':\n",
    "        data_type = sacc.standard_types.galaxy_shear_xi_plus\n",
    "    elif type_indicator == '2':\n",
    "        data_type = sacc.standard_types.galaxy_shear_xi_minus\n",
    "    else:\n",
    "        raise ValueError(\"Unknown type\")\n",
    "\n",
    "    tracer_i = f'bin_{i_bin}'\n",
    "    tracer_j = f'bin_{j_bin}'\n",
    "    \n",
    "    # This is a helper function for 2pt data specifically\n",
    "    S.add_theta_xi(data_type, tracer_i, tracer_j, theta, xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally we add in the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat_file = './KiDS-450_COSMIC_SHEAR_DATA_RELEASE/COV_MAT/xipmcutcov_KiDS-450_analytic_inc_m.dat'\n",
    "\n",
    "# Number of data points in the file.  And make the empty cov mat\n",
    "n = len(S)\n",
    "C = np.zeros((n,n))\n",
    "\n",
    "for line in open(covmat_file):\n",
    "    #Again, skip blank lines\n",
    "    line = line.strip()\n",
    "    if line.startswith('#') or not line:\n",
    "        continue\n",
    "\n",
    "    # Parse the line\n",
    "    i, j, c_ij = line.split()\n",
    "    \n",
    "    # Oh, those wacky KiDS with their one-based arrays\n",
    "    i = int(i) - 1\n",
    "    j = int(j) - 1\n",
    "    \n",
    "    # This file does have both the (equal) i,j and j,i values included.\n",
    "    C[i,j] = c_ij\n",
    "\n",
    "# Check symmetric\n",
    "assert np.allclose(C.T, C, 1e-18)\n",
    "    \n",
    "# And add.  The order matches the data vector order, according to the docs, so this should be fine.\n",
    "S.add_covariance(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And save the data\n",
    "\n",
    "We say which n(z) version we are using in the file name, for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.save_fits('kids_450_dir.sacc', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate with DES data\n",
    "\n",
    "If the DES data has also been downloaded and converted (see the other notebook in this directory)\n",
    "then we can try concatenating our files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./des-converted.sacc\"):\n",
    "    D = sacc.Sacc.load_fits('./des-converted.sacc')\n",
    "    combo = sacc.concatenate_data_sets(S, D, labels=['kids', 'des'])\n",
    "    \n",
    "    combo.save_fits(\"./des_kids_combined.sacc\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=sacc.Sacc.load_fits(\"./des_kids_combined.sacc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bin_1_kids', 'bin_2_kids', 'bin_3_kids', 'bin_4_kids', 'source_0_des', 'source_1_des', 'source_2_des', 'source_3_des', 'lens_0_des', 'lens_1_des', 'lens_2_des', 'lens_3_des', 'lens_4_des'])\n"
     ]
    }
   ],
   "source": [
    "# Bins now include both DES and KiDS sources.\n",
    "print(xx.tracers.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
