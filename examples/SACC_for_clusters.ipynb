{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SACC with clusters\n",
    "The default SACC scripts in the directory above show how one can make a SACC object for a 3x2 point analysis. The constructor for a SACC object has additional fields for handling clusters. This notebook details how one can use those fields to create/load/split a SACC that has cluster information.\n",
    "\n",
    "Note: this notebook is for *stacks* of clusters. Individual cluster measurements are not yet supported by SACC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster stack details\n",
    "Stacks of clusters are different from regular tracers, since they are binned not only in redshift but also by richness. In this example, we have 20 cluster stacks: 5 bins in richness and 4 tomographic bins. Since this is a tomographic analysis, each cluster stack can be associated with some number of source bins. This association is handled in later cells.\n",
    "\n",
    "The following two cells create two sets of tracers:\n",
    "1. cluster stack tracers, that hold tomographic and mass-proxy (aka richness) bin edges\n",
    "2. source galaxy tracers, that are associated with individual $\\gamma_T$ weak lensing profiles for each stack-souce tracer pair\n",
    "\n",
    "We could also create a new type of tracer for cluster stacks - that would make more sense in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sacc.Sacc()\n",
    "\n",
    "richness_bin_edges = [20, 30, 50, 80, 120, 180]\n",
    "source_zbin_centers = [0.5, 0.7, 0.9, 1.1]\n",
    "cluster_zbin_centers = [0.3, 0.5, 0.7, 0.9]\n",
    "nbin_richness = len(richness_bin_edges) - 1\n",
    "nbin_source = len(source_zbin_centers)\n",
    "nbin_cluster = len(cluster_zbin_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we build the cluster stack tracers.\n",
    "# Here we will store the mass information in metadata,\n",
    "# but for more complicated data we should write a new\n",
    "# subclass of Tracer to store this information.\n",
    "\n",
    "for i, z_mid in enumerate(cluster_zbin_centers):\n",
    "    z = np.arange(z_mid-0.1, z_mid+0.1, 0.001)\n",
    "    Nz = np.exp(-(z-z_mid)**2 / (2*0.03**2))\n",
    "\n",
    "    for j in range(nbin_richness):\n",
    "        l_min = richness_bin_edges[j]\n",
    "        l_max = richness_bin_edges[j+1]\n",
    "        name = f'clusters_{i}_{j}'\n",
    "        metadata = {'Mproxy_name': 'richness',\n",
    "                    'Mproxy_min': l_min, 'Mproxy_max': l_max, \n",
    "                    'source_name':'lsst_sources'\n",
    "        }\n",
    "        s.add_tracer('NZ', name, z, Nz, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we move on to the more standard galaxy tracers -\n",
    "# tomographic LSST source galaxies with 4 redshift bins\n",
    "for i,z_mid in enumerate(source_zbin_centers):\n",
    "    # Basic n(z) information\n",
    "    z = np.arange(z_mid-0.1, z_mid+0.1, 0.001)\n",
    "    Nz = np.exp(-(z-z_mid)**2 / (2*0.025**2))\n",
    "\n",
    "    # Some random shapes of Nz to marginalise over\n",
    "    # We save these as extra columns\n",
    "    DNz=np.zeros((len(Nz),2))\n",
    "    DNz[:,0]=(z-z_mid)**2*0.01\n",
    "    DNz[:,0]-=DNz[:,0].mean()\n",
    "    DNz[:,1]=(z-z_mid)**3*0.01\n",
    "    DNz[:,1]-=DNz[:,1].mean()\n",
    "    extra_columns = {'DNz_0': DNz[:,0], 'DNz_1': DNz[:,1]}\n",
    "\n",
    "    s.add_tracer(\"NZ\", f\"lsst_sources_{i}\", z, Nz, extra_columns=extra_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data vectors and binning\n",
    "The SACC holds data vectors and binning information. In this example, we have binning for cluster number counts as well as binning for cluster-source lensing profiles. Both are created in the following cell, as well as the binning information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cluster', 'galaxy'] ['density', 'shear']\n"
     ]
    }
   ],
   "source": [
    "# Here we have 10 radial bins for cluster weak lensing\n",
    "# Note that the \"radial bins\" can be actual distances or angles on the sky\n",
    "radii = np.logspace(np.log10(0.5), np.log10(3), 10)\n",
    "\n",
    "# One of our identifiers is a standard type name that is predefined\n",
    "cluster_count = sacc.standard_types.cluster_mass_count_wl\n",
    "\n",
    "# Our other one is manually defined, because it's one of those measurements\n",
    "# where people pretend to know exactly how physical scale corresponds to angle.\n",
    "# So we define our own tag for it.\n",
    "cluster_lensing = \"clusterGalaxy_densityShear_xi_tComoving\"\n",
    "# There is a standard format for these names.  We check that we fit it\n",
    "# by running the parser on it\n",
    "type_details = sacc.parse_data_type_name(cluster_lensing)\n",
    "print(type_details.sources, type_details.properties)\n",
    "\n",
    "for i in range(nbin_cluster):\n",
    "    for j in range(nbin_richness):\n",
    "        # Cluster number counts data\n",
    "        tracer1 = f'clusters_{i}_{j}'\n",
    "\n",
    "        # random data values.  For now!\n",
    "        mass= i*1e14\n",
    "        richness = 5*j\n",
    "        value = int(np.random.normal((i+10)*100, 100))\n",
    "        s.add_data_point(cluster_count, (tracer1,), value, err=100.)\n",
    "        \n",
    "        # And now the cluster lensing data points\n",
    "        for k in range(nbin_source):\n",
    "            tracer2 = f\"lsst_sources_{k}\"\n",
    "            # Separate random data values for each point\n",
    "            for r in radii:\n",
    "                value = np.random.uniform(0., 10.)\n",
    "                s.add_data_point(cluster_lensing, (tracer1, tracer2), value, radius=r, err=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrices\n",
    "Finally, the SACC object holds a covariance matrix between all of the data we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(s)\n",
    "C = np.zeros((n,n))\n",
    "\n",
    "for i in range(n):\n",
    "    di = s.data[i]\n",
    "    for j in range(n):\n",
    "        dj = s.data[j]\n",
    "        if i==j and di.data_type == cluster_count:\n",
    "            C[i,i] = di['err']**2\n",
    "        elif di.data_type == cluster_lensing:\n",
    "            C[i,j] = 0.1 * di['err'] * dj['err']\n",
    "            if i==j:\n",
    "                C[i,j] *= 10.\n",
    "        C[j,i] = C[i,j]\n",
    "\n",
    "s.add_covariance(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shuffles the data and covariance order so that it is\n",
    "# organized with all the data points of the same type collected\n",
    "# together\n",
    "s.to_canonical_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some meta data\n",
    "s.metadata['Creator'] = 'McGyver'\n",
    "s.metadata['Project'] = 'Victory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: Keyword name 'META_Mproxy_name' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n",
      "WARNING: VerifyWarning: Keyword name 'META_Mproxy_min' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n",
      "WARNING: VerifyWarning: Keyword name 'META_Mproxy_max' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n",
      "WARNING: VerifyWarning: Keyword name 'META_source_name' is greater than 8 characters or contains characters not allowed by the FITS standard; a HIERARCH card will be created. [astropy.io.fits.card]\n"
     ]
    }
   ],
   "source": [
    "s.save_fits(\"clusters.sacc\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting\n",
    "A SACC object with cluster information can be loaded and split, just like the example SACC in the 3x2pt analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_0_0',), value=1112, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_0_1',), value=1083, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_0_2',), value=1033, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_0_3',), value=949, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_0_4',), value=1028, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_1_0',), value=1185, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_1_1',), value=1018, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_1_2',), value=1038, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_1_3',), value=1172, err=100.0)\n",
      "DataPoint(data_type='cluster_mass_count_wl', tracers=('clusters_1_4',), value=985, err=100.0)\n"
     ]
    }
   ],
   "source": [
    "s2 = sacc.Sacc.load_fits(\"./clusters.sacc\")\n",
    "for d in s2.data[:10]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_mass_count_wl: 20 data points\n",
      "    clusters_0_0: 1 data points\n",
      "    clusters_0_1: 1 data points\n",
      "    clusters_0_2: 1 data points\n",
      "    clusters_0_3: 1 data points\n",
      "    clusters_0_4: 1 data points\n",
      "    clusters_1_0: 1 data points\n",
      "    clusters_1_1: 1 data points\n",
      "    clusters_1_2: 1 data points\n",
      "    clusters_1_3: 1 data points\n",
      "    clusters_1_4: 1 data points\n",
      "    clusters_2_0: 1 data points\n",
      "    clusters_2_1: 1 data points\n",
      "    clusters_2_2: 1 data points\n",
      "    clusters_2_3: 1 data points\n",
      "    clusters_2_4: 1 data points\n",
      "    clusters_3_0: 1 data points\n",
      "    clusters_3_1: 1 data points\n",
      "    clusters_3_2: 1 data points\n",
      "    clusters_3_3: 1 data points\n",
      "    clusters_3_4: 1 data points\n",
      "clusterGalaxy_densityShear_xi_tComoving: 800 data points\n",
      "    clusters_0_0-lsst_sources_0: 10 data points\n",
      "    clusters_0_0-lsst_sources_1: 10 data points\n",
      "    clusters_0_0-lsst_sources_2: 10 data points\n",
      "    clusters_0_0-lsst_sources_3: 10 data points\n",
      "    clusters_0_1-lsst_sources_0: 10 data points\n",
      "    clusters_0_1-lsst_sources_1: 10 data points\n",
      "    clusters_0_1-lsst_sources_2: 10 data points\n",
      "    clusters_0_1-lsst_sources_3: 10 data points\n",
      "    clusters_0_2-lsst_sources_0: 10 data points\n",
      "    clusters_0_2-lsst_sources_1: 10 data points\n",
      "    clusters_0_2-lsst_sources_2: 10 data points\n",
      "    clusters_0_2-lsst_sources_3: 10 data points\n",
      "    clusters_0_3-lsst_sources_0: 10 data points\n",
      "    clusters_0_3-lsst_sources_1: 10 data points\n",
      "    clusters_0_3-lsst_sources_2: 10 data points\n",
      "    clusters_0_3-lsst_sources_3: 10 data points\n",
      "    clusters_0_4-lsst_sources_0: 10 data points\n",
      "    clusters_0_4-lsst_sources_1: 10 data points\n",
      "    clusters_0_4-lsst_sources_2: 10 data points\n",
      "    clusters_0_4-lsst_sources_3: 10 data points\n",
      "    clusters_1_0-lsst_sources_0: 10 data points\n",
      "    clusters_1_0-lsst_sources_1: 10 data points\n",
      "    clusters_1_0-lsst_sources_2: 10 data points\n",
      "    clusters_1_0-lsst_sources_3: 10 data points\n",
      "    clusters_1_1-lsst_sources_0: 10 data points\n",
      "    clusters_1_1-lsst_sources_1: 10 data points\n",
      "    clusters_1_1-lsst_sources_2: 10 data points\n",
      "    clusters_1_1-lsst_sources_3: 10 data points\n",
      "    clusters_1_2-lsst_sources_0: 10 data points\n",
      "    clusters_1_2-lsst_sources_1: 10 data points\n",
      "    clusters_1_2-lsst_sources_2: 10 data points\n",
      "    clusters_1_2-lsst_sources_3: 10 data points\n",
      "    clusters_1_3-lsst_sources_0: 10 data points\n",
      "    clusters_1_3-lsst_sources_1: 10 data points\n",
      "    clusters_1_3-lsst_sources_2: 10 data points\n",
      "    clusters_1_3-lsst_sources_3: 10 data points\n",
      "    clusters_1_4-lsst_sources_0: 10 data points\n",
      "    clusters_1_4-lsst_sources_1: 10 data points\n",
      "    clusters_1_4-lsst_sources_2: 10 data points\n",
      "    clusters_1_4-lsst_sources_3: 10 data points\n",
      "    clusters_2_0-lsst_sources_0: 10 data points\n",
      "    clusters_2_0-lsst_sources_1: 10 data points\n",
      "    clusters_2_0-lsst_sources_2: 10 data points\n",
      "    clusters_2_0-lsst_sources_3: 10 data points\n",
      "    clusters_2_1-lsst_sources_0: 10 data points\n",
      "    clusters_2_1-lsst_sources_1: 10 data points\n",
      "    clusters_2_1-lsst_sources_2: 10 data points\n",
      "    clusters_2_1-lsst_sources_3: 10 data points\n",
      "    clusters_2_2-lsst_sources_0: 10 data points\n",
      "    clusters_2_2-lsst_sources_1: 10 data points\n",
      "    clusters_2_2-lsst_sources_2: 10 data points\n",
      "    clusters_2_2-lsst_sources_3: 10 data points\n",
      "    clusters_2_3-lsst_sources_0: 10 data points\n",
      "    clusters_2_3-lsst_sources_1: 10 data points\n",
      "    clusters_2_3-lsst_sources_2: 10 data points\n",
      "    clusters_2_3-lsst_sources_3: 10 data points\n",
      "    clusters_2_4-lsst_sources_0: 10 data points\n",
      "    clusters_2_4-lsst_sources_1: 10 data points\n",
      "    clusters_2_4-lsst_sources_2: 10 data points\n",
      "    clusters_2_4-lsst_sources_3: 10 data points\n",
      "    clusters_3_0-lsst_sources_0: 10 data points\n",
      "    clusters_3_0-lsst_sources_1: 10 data points\n",
      "    clusters_3_0-lsst_sources_2: 10 data points\n",
      "    clusters_3_0-lsst_sources_3: 10 data points\n",
      "    clusters_3_1-lsst_sources_0: 10 data points\n",
      "    clusters_3_1-lsst_sources_1: 10 data points\n",
      "    clusters_3_1-lsst_sources_2: 10 data points\n",
      "    clusters_3_1-lsst_sources_3: 10 data points\n",
      "    clusters_3_2-lsst_sources_0: 10 data points\n",
      "    clusters_3_2-lsst_sources_1: 10 data points\n",
      "    clusters_3_2-lsst_sources_2: 10 data points\n",
      "    clusters_3_2-lsst_sources_3: 10 data points\n",
      "    clusters_3_3-lsst_sources_0: 10 data points\n",
      "    clusters_3_3-lsst_sources_1: 10 data points\n",
      "    clusters_3_3-lsst_sources_2: 10 data points\n",
      "    clusters_3_3-lsst_sources_3: 10 data points\n",
      "    clusters_3_4-lsst_sources_0: 10 data points\n",
      "    clusters_3_4-lsst_sources_1: 10 data points\n",
      "    clusters_3_4-lsst_sources_2: 10 data points\n",
      "    clusters_3_4-lsst_sources_3: 10 data points\n"
     ]
    }
   ],
   "source": [
    "# Printing a summary\n",
    "for dt in s2.get_data_types():\n",
    "    ind = s2.indices(dt)\n",
    "    n = len(ind)\n",
    "    print(f\"{dt}: {n} data points\")\n",
    "    for tracers in s2.get_tracer_combinations(dt):\n",
    "        ind = s2.indices(dt, tracers)\n",
    "        n = len(ind)\n",
    "        tracers = '-'.join(tracers)\n",
    "        print(f\"    {tracers}: {n} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 800\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into two parts\n",
    "s3 = s2.copy()\n",
    "s4 = s2.copy()\n",
    "s3.keep_selection(cluster_count)\n",
    "s4.keep_selection(cluster_lensing)\n",
    "print(len(s3), len(s4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
